{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Evaluation Context Protocol (ECP)","text":"<p>View on GitHub | README</p> <p>ECP is a lightweight protocol and reference runtime for evaluating agents with public output, private reasoning, and tool usage.</p> <p>It gives you a standard way to run deterministic evaluations without changing your production agent code.</p>"},{"location":"#why-ecp-exists","title":"Why ECP exists","text":"<p>Most agent evaluations only check the final answer. That is not enough for safety or reliability.</p> <p>Common gaps:</p> <ul> <li>Did the agent use the right tool or hallucinate data?</li> <li>Did it follow policy internally before responding?</li> <li>Did it reason correctly even if the final answer looks right?</li> </ul> <p>ECP solves this by separating public output (what users see) from private reasoning and tool calls (what evaluators need to verify). The runtime can then grade each aspect explicitly.</p>"},{"location":"#what-you-get","title":"What you get","text":"<ul> <li>A simple JSON-RPC protocol over stdio</li> <li>A reference runtime to execute manifests and graders</li> <li>Optional HTML report output for sharing results</li> <li>A Python SDK to wrap agents quickly</li> <li>Minimal examples for LangChain and LlamaIndex</li> </ul>"},{"location":"#framework-adaptors","title":"Framework Adaptors","text":"<ul> <li>LangChain: <code>ecp.adaptors.langchain.ECPLangChainAdapter</code></li> <li>LlamaIndex: <code>ecp.adaptors.llama_index.ECPLlamaIndexAdapter</code></li> </ul> <p>See Examples for full agent + manifest snippets.</p>"},{"location":"#what-is-in-this-repo","title":"What is in this repo","text":"<ul> <li>Python SDK: <code>sdk/python/src/ecp</code></li> <li>Runtime CLI: <code>runtime/python/src/ecp_runtime</code></li> <li>Examples: <code>examples/langchain_demo</code>, <code>examples/llamaindex_demo</code></li> <li>Protocol spec: <code>spec/protocol.md</code></li> </ul> <p>Go to Quickstart to run the demos, Examples for full manifests, or Specification to implement the protocol in another language.</p>"},{"location":"examples/","title":"Examples","text":"<p>View on GitHub | Docs Home</p>"},{"location":"examples/#langchain-example","title":"LangChain Example","text":"<p>Agent file: <code>examples/langchain_demo/agent.py</code></p>"},{"location":"examples/#langchain-manifest-full-scenarios","title":"LangChain Manifest (Full Scenarios)","text":"<pre><code>manifest_version: \"v1\"\nname: \"LangChain Math Check\"\ntarget: \"python agent.py\"\n\nscenarios:\n  - name: \"Ratio Word Problem\"\n    steps:\n      - input: \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used.\"\n        graders:\n          - type: text_match\n            field: public_output\n            condition: contains\n            value: \"42\"\n\n          - type: llm_judge\n            field: public_output\n            prompt: \"Does the response state the correct final number and clearly indicate it refers to the amount of sugar?\"\n\n          - type: tool_usage\n            tool_name: \"calculator\"\n            arguments: {}\n\n  - name: \"Weekly Letters\"\n    steps:\n      - input: \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\"\n        graders:\n          - type: text_match\n            field: public_output\n            condition: contains\n            value: \"624\"\n\n          - type: llm_judge\n            field: public_output\n            prompt: \"Does the response state the correct final number and clearly indicate it refers to pages written per year?\"\n\n          - type: tool_usage\n            tool_name: \"calculator\"\n            arguments: {}\n</code></pre>"},{"location":"examples/#llamaindex-example","title":"LlamaIndex Example","text":"<p>Agent file: <code>examples/llamaindex_demo/agent.py</code></p> <pre><code>from llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n\nfrom ecp import serve\nfrom ecp.adaptors.llama_index import ECPLlamaIndexAdapter\n\n\ndef multiply(a: float, b: float) -&gt; float:\n    return a * b\n\n\ndef add(a: float, b: float) -&gt; float:\n    return a + b\n\n\nfinance_tools = YahooFinanceToolSpec().to_tool_list()\nfinance_tools.extend([multiply, add])\n\nworkflow = FunctionAgent(\n    name=\"Agent\",\n    description=\"Useful for performing financial operations.\",\n    llm=OpenAI(model=\"gpt-4o-mini\"),\n    tools=[multiply, add],\n    system_prompt=\"You are a helpful assistant.\",\n)\n\necp_agent = ECPLlamaIndexAdapter(workflow, name=\"FinanceBot\")\n\nif __name__ == \"__main__\":\n    serve(ecp_agent)\n</code></pre>"},{"location":"examples/#llamaindex-manifest","title":"LlamaIndex Manifest","text":"<pre><code>manifest_version: \"v1\"\nname: \"LlamaIndex Capability Check\"\ntarget: \"python agent.py\"\n\nscenarios:\n  - name: \"Ratio Word Problem\"\n    steps:\n      - input: \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used.\"\n        graders:\n          - type: text_match\n            field: public_output\n            condition: contains\n            value: \"42\"\n\n          - type: llm_judge\n            field: public_output\n            prompt: \"Does the response state the correct final number and clearly indicate it refers to the amount of sugar?\"\n\n  - name: \"Weekly Letters\"\n    steps:\n      - input: \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\"\n        graders:\n          - type: text_match\n            field: public_output\n            condition: contains\n            value: \"312\"\n\n          - type: llm_judge\n            field: public_output\n            prompt: \"Does the response state the correct final number and clearly indicate it refers to pages written per year?\"\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>View on GitHub | Docs Home</p>"},{"location":"quickstart/#1-create-a-venv-and-install-from-pypi","title":"1. Create a venv and install from PyPI","text":"<pre><code>py -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\npip install ecp-runtime \"ecp-sdk[langchain]\" langchain-openai\n</code></pre>"},{"location":"quickstart/#2-run-the-demo","title":"2. Run the demo","text":"<pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml\n</code></pre>"},{"location":"quickstart/#3-generate-an-html-report","title":"3. Generate an HTML report","text":"<pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml --report .\\report.html\n</code></pre>"},{"location":"quickstart/#4-json-output-for-ci","title":"4. JSON output (for CI)","text":"<p>Print a JSON report to stdout:</p> <pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml --json\n</code></pre> <p>Save a JSON report to a file:</p> <pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml --json-out .\\report.json\n</code></pre>"},{"location":"quickstart/#5-optional-enable-llm-judge","title":"5. Optional: enable LLM judge","text":"<p>If your manifest uses <code>llm_judge</code>, set the API key:</p> <pre><code>$env:OPENAI_API_KEY=\"your_key_here\"\n</code></pre>"},{"location":"quickstart/#notes","title":"Notes","text":"<ul> <li>The runtime launches your agent via the <code>target</code> command in the manifest.</li> <li>The agent responds over JSON-RPC 2.0 on stdio.</li> <li>Use <code>ECP_RPC_TIMEOUT</code> to control step timeouts (default 30s).</li> </ul>"},{"location":"spec/","title":"Specification","text":"<p>View on GitHub | Protocol Source</p>"},{"location":"spec/#overview","title":"Overview","text":"<p>ECP is JSON-RPC 2.0 over stdio. The runtime spawns the agent process and sends requests for <code>initialize</code>, <code>step</code>, and <code>reset</code>. The agent returns structured results containing public output, private reasoning, and tool usage.</p>"},{"location":"spec/#methods","title":"Methods","text":""},{"location":"spec/#agentinitialize","title":"agent/initialize","text":"<p>Params: <code>config</code> (object, optional)</p> <p>Result: <code>{ name, capabilities }</code></p>"},{"location":"spec/#agentstep","title":"agent/step","text":"<p>Params: <code>input</code> (string)</p> <p>Result:</p> <ul> <li><code>status</code>: <code>done</code> or <code>paused</code></li> <li><code>public_output</code>: string or null</li> <li><code>private_thought</code>: string or null</li> <li><code>tool_calls</code>: array or null</li> </ul> <p>Tool call format:</p> <pre><code>{ \"name\": \"calculator\", \"arguments\": { \"expression\": \"2+2\" } }\n</code></pre>"},{"location":"spec/#agentreset","title":"agent/reset","text":"<p>Params: none</p> <p>Result: <code>true</code></p>"},{"location":"spec/#manifest","title":"Manifest","text":"<p>The runtime reads a YAML manifest describing scenarios and graders.</p> <p>Supported graders:</p> <ul> <li><code>text_match</code> (contains, equals, does_not_contain, regex)</li> <li><code>llm_judge</code> (requires <code>OPENAI_API_KEY</code>)</li> <li><code>tool_usage</code> (name + argument subset match)</li> </ul> <p>See <code>examples/langchain_demo/manifest.yaml</code> for a minimal example.</p>"},{"location":"spec/#reports","title":"Reports","text":"<p>The runtime can optionally generate a single HTML report for a run:</p> <pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml --report .\\report.html\n</code></pre> <p>The runtime can also emit a JSON report (useful for CI):</p> <pre><code>python -m ecp_runtime.cli run --manifest .\\examples\\langchain_demo\\manifest.yaml --json\n</code></pre>"}]}